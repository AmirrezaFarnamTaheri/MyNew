{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff55457",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29a9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f187671",
   "metadata": {},
   "source": [
    "\n",
    "#### Introduction\n",
    "\n",
    ">   Sentiment analysis systems, as one of the most important applications of natural language processing, play a vital role in automated understanding of emotions and opinions in texts. These systems are now used in various fields such as social surveys, customer feedback analysis, brand reputation management, and even market research.\n",
    ">   \n",
    ">   Large language models like GPT have created a remarkable transformation in the field of natural language processing. These models, with their transformer-based architecture and training on vast amounts of textual data, acquire the ability to understand the most complex language patterns. The GPT model family, from GPT-1 to GPT-4, with self-supervised learning and unidirectional attention, have the ability to generate coherent texts and answer questions.\n",
    ">   \n",
    ">   In this exercise, we use GPT-2 (a balanced version in terms of size and efficiency) to perform sentiment analysis on the IMDb dataset. Unlike traditional methods that use specialized classifiers like BERT, here we formulate the problem as conditional text generation; meaning that after reading a review, the model generates the word \"positive\" or \"negative\" as a response.\n",
    ">   \n",
    ">   The purpose of this exercise is to become familiar with fine-tuning pre-trained models, understand the differences between generative versus discriminative approaches, and examine the challenges of using GPT for classification tasks. Finally, we will evaluate the trained model with Accuracy and Perplexity metrics (which measure the model's confidence in predictions) and compare the results with baseline methods.\n",
    ">   \n",
    "#### 2-1. Paper Introduction (10 points)\n",
    ">   \n",
    ">   To become familiar with the theoretical foundations of transformer models, particularly the GPT architecture, read the paper from [this link](https://arxiv.org/abs/2005.14165) and answer the following in your report:\n",
    ">   1. How does the unidirectional attention architecture in GPT work?\n",
    ">   2. What are the key differences between GPT and BERT architectures?\n",
    ">   3. How are pre-training and fine-tuning methods performed in GPT?\n",
    ">   \n",
    "#### 2-2. Data Preprocessing (30 points)\n",
    ">   \n",
    ">   In this section, we use the IMDb dataset which includes 50,000 movie reviews with positive and negative labels. This dataset is accessible through the datasets library:\n",
    ">   ```python\n",
    ">   # Load and prepare dataset\n",
    ">   dataset = load_dataset(\"imdb\")\n",
    ">   ```\n",
    ">   \n",
    ">   a) Examine and display the class distribution, analyze text length with mean, median, maximum, and minimum metrics, and show a sample of raw data. For simplicity, you can use shorter reviews (minimum 500 characters).\n",
    ">   \n",
    ">   b) For formatting the data, use this format:\n",
    ">   ```\n",
    ">   \"Review: {review text}\\nSentiment: {label}\"\n",
    ">   ```\n",
    ">   Convert numerical labels to text:\n",
    ">   ```python\n",
    ">   1 → \"positive\", 0 → \"negative\"\n",
    ">   ```\n",
    ">   \n",
    ">   c) Use the GPT-2 tokenizer for tokenizing data:\n",
    ">   ```python\n",
    ">   tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    ">   tokenizer.pad_token = tokenizer.eos_token\n",
    ">   ```\n",
    ">   \n",
    ">   d) Create a data loader, find the appropriate batch size, and you can access these data by calling the train and test columns. To speed up the work, use 5000 samples from the train data and 1000 samples from the test data.\n",
    ">   \n",
    "#### 2-3. Model Implementation (50 points)\n",
    ">   \n",
    ">   In this part of the project, you are responsible for implementing a GPT-2 based model. This model should be implemented according to its specific architecture and the following important points:\n",
    ">   ```python\n",
    ">   model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    ">   ```\n",
    ">   \n",
    ">   GPT-2 is one of the famous and powerful models in natural language processing. To start, you should use a pre-trained version of this model. This allows you to take advantage of the model's pre-learned knowledge and focus more on training the upper layers.\n",
    ">   \n",
    ">   One of the key points in this implementation is freezing the base layers of the model. With this, you will only train the upper (thinner) layers. This method usually helps increase training speed and prevent overfitting. By freezing the base layers, you can ensure that the model's basic features are preserved and only more specific aspects of the data are learned.\n",
    ">   \n",
    ">   To optimize the model's performance, adjusting hyperparameters such as learning rate, number of epochs, and batch sizes is very important. You should find the best combination of these parameters through initial experiments.\n",
    ">   \n",
    ">   In this implementation, the loss function should only be calculated for the labels. This means that you should focus only on the target data and ensure that the model is optimized for learning this data.\n",
    "\n",
    "#### 2-4. Evaluation and Analysis of Results (10 points)\n",
    "\n",
    ">   Now evaluate the model with accuracy metrics (which show how much of the model's predictions match the actual labels) and perplexity (which measures the model's confidence in predictions and the lower it is, the higher the confidence). Also, after calculating Recall and Precision, display its confusion matrix.\n",
    ">   \n",
    ">   Explain what the difference is between this method and using models like BERT, which is specifically built for classification? And examine the strengths and weaknesses of each method.\n",
    "\n",
    "#### 2-5. Bonus (10 points)\n",
    "\n",
    "\n",
    ">   Another approach for training pre-trained GPT models is using the LoRA (Low-Rank Adaptation) method with fewer resources while maintaining performance. LoRA adds trainable low-rank matrices to the model's original weights, allowing the model to be tuned without directly changing the pre-trained weights, reducing the number of parameters that need to be updated. Instead of freezing layers and limited use of the last two layers, try implementing the exercise with this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Configuration ----------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    model_name: str = \"gpt2\"\n",
    "    max_length: int = 512\n",
    "    batch_size: int = 16\n",
    "    epochs: int = 3\n",
    "    lr: float = 3e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    grad_accum: int = 2\n",
    "    grad_clip: float = 1.0\n",
    "    seed: int = 42\n",
    "\n",
    "    # data\n",
    "    train_samples: int = 5_000\n",
    "    test_samples: int = 1_000\n",
    "    min_text_len: int = 100\n",
    "\n",
    "    # LoRA\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "\n",
    "    # device\n",
    "    device: torch.device = (\n",
    "        torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Reproducibility helpers -------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Data --------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def analyse_imdb(ds):\n",
    "    train, test = ds[\"train\"], ds[\"test\"]\n",
    "    neg, pos = np.bincount(train[\"label\"])\n",
    "    print(\n",
    "        f\"Train: {len(train):,}  Test: {len(test):,}  «neg» {neg:,}  «pos» {pos:,}\"\n",
    "    )\n",
    "\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    \"\"\"Simple dataset → returns dict with input_ids, attention_mask, labels, label\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rows: List[dict],\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        max_len: int,\n",
    "        training: bool = True,\n",
    "    ):\n",
    "        self.rows = rows\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.training = training\n",
    "\n",
    "        # cache helper tokens\n",
    "        self.positive_id = self.tok.encode(\"positive\", add_special_tokens=False)[0]\n",
    "        self.negative_id = self.tok.encode(\"negative\", add_special_tokens=False)[0]\n",
    "        self.sentinel_ids = self.tok.encode(\"Sentiment:\", add_special_tokens=False)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _find_sublist(haystack: List[int], needle: List[int]) -> int | None:\n",
    "        \"\"\"Return index of *first* element of needle in haystack or None.\"\"\"\n",
    "        for i in range(len(haystack) - len(needle) + 1):\n",
    "            if haystack[i : i + len(needle)] == needle:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        label_word = \"positive\" if label == 1 else \"negative\"\n",
    "        prompt = f\"Review: {text}\\nSentiment: {label_word}\"\n",
    "\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=False,  # GPT‑2 has no BOS/EOS tokens\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc.input_ids.squeeze(0)\n",
    "        attention = enc.attention_mask.squeeze(0)\n",
    "\n",
    "        # label masking\n",
    "        if self.training:\n",
    "            labels = torch.full_like(input_ids, -100)\n",
    "            idx0 = self._find_sublist(input_ids.tolist(), self.sentinel_ids)\n",
    "            if idx0 is not None and idx0 + len(self.sentinel_ids) < len(labels):\n",
    "                target_pos = idx0 + len(self.sentinel_ids)  # first token after \"Sentiment:\"\n",
    "                labels[target_pos] = (\n",
    "                    self.positive_id if label == 1 else self.negative_id\n",
    "                )\n",
    "        else:\n",
    "            # for validation we just compute LM loss across sequence\n",
    "            labels = input_ids.clone()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention,\n",
    "            \"labels\": labels,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. LoRA helper -------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"LoRA wrapper per https://arxiv.org/abs/2106.09685\"\"\"\n",
    "\n",
    "    def __init__(self, base: nn.Linear, r: int, alpha: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.r = r\n",
    "        self.scale = alpha / r\n",
    "        self.drop = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        in_f, out_f = base.in_features, base.out_features\n",
    "        # Init as suggested → A random, B zeros\n",
    "        self.A = nn.Parameter(torch.randn(r, in_f) * (1 / math.sqrt(r)))\n",
    "        self.B = nn.Parameter(torch.zeros(out_f, r))\n",
    "\n",
    "        # mark so we can pick trainable params\n",
    "        self.A._is_lora = True\n",
    "        self.B._is_lora = True\n",
    "\n",
    "        # freeze base weights\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x) + (self.drop(x) @ self.A.T @ self.B.T) * self.scale\n",
    "\n",
    "\n",
    "class GPT2WithLoRA(nn.Module):\n",
    "    \"\"\"GPT‑2 with LoRA applied to qkv and proj matrices.\"\"\"\n",
    "\n",
    "    TARGETS = (\"c_attn\", \"c_proj\")  # names used inside GPT‑2 blocks\n",
    "\n",
    "    def __init__(self, r: int, alpha: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.base = GPT2LMHeadModel.from_pretrained(cfg.model_name)\n",
    "        self._apply_lora(r, alpha, dropout)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _apply_lora(self, r: int, alpha: int, dropout: float):\n",
    "        rep_names = [n for n, m in self.base.named_modules() if isinstance(m, nn.Linear) and any(t in n for t in self.TARGETS)]\n",
    "        for name in rep_names:\n",
    "            parent_name, child_name = name.rsplit(\".\", 1)\n",
    "            parent_mod = dict(self.base.named_modules())[parent_name]\n",
    "            orig_layer: nn.Linear = getattr(parent_mod, child_name)\n",
    "            setattr(parent_mod, child_name, LoRALinear(orig_layer, r, alpha, dropout))\n",
    "        print(f\"LoRA applied to {len(rep_names)} linear layers (rank={r}).\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def forward(self, **kwargs):\n",
    "        return self.base(**kwargs)\n",
    "\n",
    "    # convenience\n",
    "    def parameters_trainable(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Training utils ----------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def build_loaders(tokenizer: GPT2Tokenizer) -> Tuple[DataLoader, DataLoader, List[dict]]:\n",
    "    ds_full = load_dataset(\"imdb\")\n",
    "    analyse_imdb(ds_full)\n",
    "\n",
    "    # filter + balanced subsample\n",
    "    train_rows = [r for r in ds_full[\"train\"] if len(r[\"text\"]) >= cfg.min_text_len]\n",
    "    test_rows = [r for r in ds_full[\"test\"] if len(r[\"text\"]) >= cfg.min_text_len]\n",
    "\n",
    "    random.shuffle(train_rows)\n",
    "    random.shuffle(test_rows)\n",
    "\n",
    "    # balanced sample train\n",
    "    pos_rows = [r for r in train_rows if r[\"label\"] == 1][: cfg.train_samples // 2]\n",
    "    neg_rows = [r for r in train_rows if r[\"label\"] == 0][: cfg.train_samples // 2]\n",
    "    train_sel = pos_rows + neg_rows\n",
    "    random.shuffle(train_sel)\n",
    "\n",
    "    test_sel = test_rows[: cfg.test_samples]\n",
    "\n",
    "    train_ds = IMDbDataset(train_sel, tokenizer, cfg.max_length, training=True)\n",
    "    test_ds = IMDbDataset(test_sel, tokenizer, cfg.max_length, training=False)\n",
    "\n",
    "    loader_kw = dict(\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        DataLoader(train_ds, shuffle=True, **loader_kw),\n",
    "        DataLoader(test_ds, shuffle=False, **loader_kw),\n",
    "        test_sel,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6. Trainer -----------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, tokenizer: GPT2Tokenizer):\n",
    "        self.model = model.to(cfg.device)\n",
    "        self.tok = tokenizer\n",
    "\n",
    "        self.scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "        self.opt = AdamW(\n",
    "            model.parameters_trainable() if hasattr(model, \"parameters_trainable\") else (p for p in model.parameters() if p.requires_grad),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "        )\n",
    "        self.scheduler = None  # filled later\n",
    "\n",
    "        # cache ids once\n",
    "        self.pos_id = self.tok.encode(\"positive\", add_special_tokens=False)[0]\n",
    "        self.neg_id = self.tok.encode(\"negative\", add_special_tokens=False)[0]\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    def loss_fn(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            ignore_index=-100,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    def train_epoch(self, loader: DataLoader):\n",
    "        self.model.train()\n",
    "        total, steps = 0.0, 0\n",
    "        for step, batch in enumerate(tqdm(loader, desc=\"train\")):\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(cfg.device)\n",
    "\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                out = self.model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                )\n",
    "                loss = self.loss_fn(out.logits, batch[\"labels\"]) / cfg.grad_accum\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if (step + 1) % cfg.grad_accum == 0:\n",
    "                self.scaler.unscale_(self.opt)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), cfg.grad_clip)\n",
    "                self.scaler.step(self.opt)\n",
    "                self.scaler.update()\n",
    "                self.opt.zero_grad(True)\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "            total += loss.item() * cfg.grad_accum\n",
    "            steps += 1\n",
    "        return total / steps\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict_batch(self, texts: List[str], bs: int = 8) -> List[int]:\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        for i in range(0, len(texts), bs):\n",
    "            batch_texts = texts[i : i + bs]\n",
    "            prompts = [f\"Review: {t}\\nSentiment:\" for t in batch_texts]\n",
    "            enc = self.tok(\n",
    "                prompts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=cfg.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(cfg.device)\n",
    "            logits = self.model(**enc).logits\n",
    "\n",
    "            # index of last non‑pad token per sample\n",
    "            last = enc.attention_mask.sum(1) - 1\n",
    "            for j, l in enumerate(last):\n",
    "                logit = logits[j, l, :]\n",
    "                pred = 1 if (logit[self.pos_id] - logit[self.neg_id]).item() > 0 else 0\n",
    "                preds.append(pred)\n",
    "        return preds\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    def validate(self, loader: DataLoader, raw_rows: List[dict]):\n",
    "        self.model.eval()\n",
    "        # LM loss on validation split\n",
    "        v_loss, n = 0.0, 0\n",
    "        for batch in loader:\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(cfg.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                )\n",
    "                v_loss += self.loss_fn(out.logits, batch[\"labels\"]).item()\n",
    "                n += 1\n",
    "        v_loss /= n\n",
    "        try:\n",
    "            ppl = math.exp(min(v_loss, 10))\n",
    "        except OverflowError:\n",
    "            ppl = float(\"inf\")\n",
    "\n",
    "        preds = self.predict_batch([r[\"text\"] for r in raw_rows])\n",
    "        true = [r[\"label\"] for r in raw_rows]\n",
    "        acc = accuracy_score(true, preds)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(true, preds, average=\"weighted\")\n",
    "        return {\n",
    "            \"loss\": v_loss,\n",
    "            \"perplexity\": ppl,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "            \"preds\": preds,\n",
    "            \"true\": true,\n",
    "        }\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 7. Main ------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(cfg.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # needed for padding\n",
    "train_loader, val_loader, val_rows = build_loaders(tokenizer)\n",
    "if args.use_lora:\n",
    "    model = GPT2WithLoRA(cfg.lora_rank, cfg.lora_alpha, cfg.lora_dropout)\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained(cfg.model_name)\n",
    "    # freeze all but last 2 transformer layers\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for block in model.transformer.h[-2:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "print(\n",
    "    f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,} /\"\n",
    "    f\" {sum(p.numel() for p in model.parameters()):,}\"\n",
    ")\n",
    "trainer = Trainer(model, tokenizer)\n",
    "total_steps = len(train_loader) * cfg.epochs // cfg.grad_accum\n",
    "warmup = int(cfg.warmup_ratio * total_steps)\n",
    "trainer.scheduler = get_linear_schedule_with_warmup(\n",
    "    trainer.opt, warmup, total_steps\n",
    ")\n",
    "best_acc = 0.0\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{cfg.epochs}\")\n",
    "    train_loss = trainer.train_epoch(train_loader)\n",
    "    metrics = trainer.validate(val_loader, val_rows)\n",
    "    print(\n",
    "        f\"train_loss {train_loss:.4f}  val_loss {metrics['loss']:.4f}  \"\n",
    "        f\"acc {metrics['accuracy']:.3f}  f1 {metrics['f1']:.3f}  \"\n",
    "        f\"ppl {metrics['perplexity']:.2f}\"\n",
    "    )\n",
    "    # save best\n",
    "    if metrics[\"accuracy\"] > best_acc:\n",
    "        best_acc = metrics[\"accuracy\"]\n",
    "        ckpt = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"tok\": tokenizer,\n",
    "            \"cfg\": cfg.__dict__,\n",
    "        }\n",
    "        torch.save(ckpt, \"best_lora_gpt2.pt\")\n",
    "        print(\"✓ saved best model\")\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "  # confusion matrix\n",
    "  cm = confusion_matrix(metrics[\"true\"], metrics[\"preds\"])\n",
    "  print(\"Confusion matrix:\")\n",
    "  print(cm)\n",
    "\n",
    "\n",
    "\n",
    "p = argparse.ArgumentParser()\n",
    "p.add_argument(\"--use_lora\", dest=\"use_lora\", action=\"store_true\", help=\"train with LoRA (default)\")\n",
    "p.add_argument(\"--no_lora\", dest=\"use_lora\", action=\"store_false\", help=\"fine‑tune GPT‑2 head only\")\n",
    "p.set_defaults(use_lora=True)\n",
    "p.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0545401",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
